{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USGS API Query for DEM 1/9 Arc resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initiating API Query:: 2023-12-04 12:49:03.358439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392788.4844000004 4704571.299900001 498367.7719000004 4724822.046900001\n",
      "-88.30832095063823 42.48607514947249 -87.01986291640718 42.67587109009426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total items returned from API Query: 4\n",
      "INFO:__main__:Total items added to download list: 4\n",
      "INFO:__main__:Initiating TIFF file download from USGS:: 2023-12-04 12:49:05.195333\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS NED ned19_n42x50_w088x00_il_lakeco_2007 1/9 arc-second 2011 15 x 15 minute IMG\n",
      "1 of 4 items to download\n",
      "Respone Query Index : 1\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS NED ned19_n42x50_w088x25_il_lakeco_2007 1/9 arc-second 2011 15 x 15 minute IMG\n",
      "2 of 4 items to download\n",
      "Respone Query Index : 2\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS NED ned19_n42x50_w088x25_il_mchenryco_2008 1/9 arc-second 2009 15 x 15 minute IMG\n",
      "3 of 4 items to download\n",
      "Respone Query Index : 3\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS NED ned19_n42x50_w088x50_il_mchenryco_2008 1/9 arc-second 2009 15 x 15 minute IMG\n",
      "4 of 4 items to download\n",
      "Respone Query Index : 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import requests\n",
    "import logging\n",
    "import errno\n",
    "import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from datetime import datetime\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://tnmaccess.nationalmap.gov/api/v1/products?datasets=National%20Elevation%20Dataset%20(NED)%201/9%20arc-second&max=999000\"\n",
    "\n",
    "# Define the query parameters\n",
    "\n",
    "# Set the feature class or shapefile path\n",
    "\n",
    "feature_class = r\"C:\\Project_Files\\GIS\\HMP+\\Edenton\\4.0 Reference Data\\USGS\\LiDAR_Processing\\Working_Data\\Other_Data\\edenton_bb.shp\"\n",
    "\n",
    "# Set the file download Directory\n",
    "download_directory = r\"C:\\Project_Files\\GIS\\HMP+\\Edenton\\4.0 Reference Data\\USGS\\LiDAR_Processing\\Working_Data\\Working_DEM\\19arc\"\n",
    "\n",
    "# Define the date range (start date and end date)\n",
    "start_date_str = \"2010-01-01\"\n",
    "end_date_str = \"2023-05-01\"\n",
    "\n",
    "# Convert start_date_str and end_date_str to datetime objects\n",
    "start_date_datetime = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "end_date_datetime = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "\n",
    "# Set log file \n",
    "SaveLogsTo = os.path.join(download_directory, \"Download_Log\")\n",
    "\n",
    "#Setup logging - levels are DEBUG,INFO,WARNING,ERROR,CRITICAL\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "''' ********************** SCRIPT CONFIGURATION END ********************** '''\n",
    "\n",
    "#https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist\n",
    "def createFolder(folderPath):\n",
    "    if not os.path.exists(folderPath):\n",
    "        try:\n",
    "            os.makedirs(folderPath)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "#Create specified folder if it does not exist already\n",
    "createFolder(SaveLogsTo)\n",
    "\n",
    "#Logging level specified in script configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logFileName = datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "fileHandler = logging.handlers.RotatingFileHandler('{}/{}.log'.format(SaveLogsTo, logFileName), maxBytes=100000, backupCount=5)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(relativeCreated)d \\n%(filename)s %(module)s %(funcName)s %(lineno)d \\n%(message)s\\n')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "offset = 0 # offset used to run with response query index offset - usually when download request times out or disconnects\n",
    "items_to_download =[]\n",
    "item_responses = 0\n",
    "\n",
    "# Get the spatial extent of the feature class\n",
    "desc = arcpy.Describe(feature_class)\n",
    "spatial_extent = desc.extent\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin = spatial_extent.XMin\n",
    "ymin = spatial_extent.YMin\n",
    "xmax = spatial_extent.XMax\n",
    "ymax = spatial_extent.YMax\n",
    "\n",
    "print(xmin, ymin, xmax, ymax)\n",
    "\n",
    "extent_dd = spatial_extent.projectAs(arcpy.SpatialReference(4326))\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin1 = extent_dd.XMin\n",
    "ymin1 = extent_dd.YMin\n",
    "xmax1 = extent_dd.XMax\n",
    "ymax1 = extent_dd.YMax\n",
    "\n",
    "print(xmin1, ymin1, xmax1, ymax1)\n",
    "\n",
    "params = {\n",
    "    \"bbox\": f\"{xmin1},{ymin1},{xmax1},{ymax1}\"\n",
    "}\n",
    "\n",
    "logger.info(\"Initiating API Query:: {0}\".format(str(datetime.now())))\n",
    "while True:\n",
    "    # Create the API URL with the current offset\n",
    "    url = f\"{url}&offset={offset}\"\n",
    "\n",
    "    # Send the API request and retrieve the response\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()[\"items\"]\n",
    "\n",
    "        # If there are no items in the response, we have retrieved all items\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "\n",
    "        for index, item in enumerate(data):\n",
    "            item_responses += 1\n",
    "            pub_date = datetime.strptime(item[\"publicationDate\"], \"%Y-%m-%d\")\n",
    "            download_url = item[\"downloadURL\"]\n",
    "            if start_date_datetime <= pub_date <= end_date_datetime:\n",
    "                if download_url not in [item[\"download_url\"] for item in items_to_download]: # only unique instances of the download url will be added to the dictionary from the query results\n",
    "                    items_to_download.append({\n",
    "                        \"publication_date\": item[\"publicationDate\"],\n",
    "                        \"download_url\": item[\"downloadURL\"],\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"response_index\": index + 1})\n",
    "            \n",
    "        # Increment the offset for the next page of items\n",
    "        offset += 999\n",
    "\n",
    "logger.info(f\"Total items returned from API Query: {item_responses}\")\n",
    "logger.info(f\"Total items added to download list: {len(items_to_download)}\")\n",
    "\n",
    "# Iterate over each item in the items to download list and download them\n",
    "logger.info(\"Initiating TIFF file download from USGS:: {0}\".format(datetime.now()))\n",
    "for index, item in enumerate(items_to_download):\n",
    "    publication_date = item[\"publication_date\"]\n",
    "    download_url = item[\"download_url\"]\n",
    "    title = item[\"title\"]\n",
    "    response = item[\"response_index\"]\n",
    "\n",
    "    # Specify the local file path to save the downloaded file\n",
    "    rename = title.replace(\" \",\"_\") # replace spaces in name with underscores\n",
    "    renamex = rename.replace(\"/\",\"_\")\n",
    "    save_path = os.path.join(download_directory, renamex + \".tif\")\n",
    "\n",
    "        # Download the file and save it locally\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(requests.get(download_url).content)\n",
    "        logger.info(f\"saved item : : \\nTitle: {title}\\n{index + 1} of {len(items_to_download)} items to download\\nRespone Query Index : {response}\")\n",
    "        arcpy.SetProgressorPosition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USGS API Query for DEM 1/3 Arc resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initiating API Query:: 2023-11-01 14:03:05.750694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508800.55179999955 5054368.9169 575494.5899 5111996.793400001\n",
      "-92.88706968404222 45.63876090354563 -92.0222473421357 46.16149077419841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total items returned from API Query: 6\n",
      "INFO:__main__:Total items added to download list: 6\n",
      "INFO:__main__:Initiating TIFF file download from USGS:: 2023-11-01 14:03:07.326637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USGS 1/3 Arc Second n46w093 20211124\n",
      "USGS 1/3 Arc Second n46w093 20230210\n",
      "USGS 1/3 Arc Second n47w093 20211124\n",
      "USGS 1/3 Arc Second n47w093 20221115\n",
      "USGS 1/3 arc-second n46w093 1 x 1 degree\n",
      "USGS 1/3 arc-second n47w093 1 x 1 degree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 Arc Second n46w093 20211124\n",
      "1 of 6 items to download\n",
      "Respone Query Index : 1\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 Arc Second n46w093 20230210\n",
      "2 of 6 items to download\n",
      "Respone Query Index : 2\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 Arc Second n47w093 20211124\n",
      "3 of 6 items to download\n",
      "Respone Query Index : 3\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 Arc Second n47w093 20221115\n",
      "4 of 6 items to download\n",
      "Respone Query Index : 4\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 arc-second n46w093 1 x 1 degree\n",
      "5 of 6 items to download\n",
      "Respone Query Index : 5\n",
      "INFO:__main__:saved item : : \n",
      "Title: USGS 1/3 arc-second n47w093 1 x 1 degree\n",
      "6 of 6 items to download\n",
      "Respone Query Index : 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import requests\n",
    "import logging\n",
    "import errno\n",
    "import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from datetime import datetime\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://tnmaccess.nationalmap.gov/api/v1/products?datasets=National%20Elevation%20Dataset%20(NED)%201/3%20arc-second&max=999000\"\n",
    "\n",
    "# Define the query parameters\n",
    "\n",
    "# Set the feature class or shapefile path\n",
    "\n",
    "feature_class = r\"C:\\Project_Files\\GIS\\FFRMS+\\Counties+\\OH_39027+\\Working_Data\\Other_Data\\Clinton_17.shp\"\n",
    "\n",
    "# Set the file download Directory\n",
    "download_directory = r\"C:\\Project_Files\\GIS\\FFRMS+\\Counties+\\OH_39027+\\Working_Data\\Working_DEM\\01m\"\n",
    "\n",
    "# Define the date range (start date and end date)\n",
    "start_date_str = \"2020-02-01\"\n",
    "end_date_str = \"2020-04-01\"\n",
    "\n",
    "# Convert start_date_str and end_date_str to datetime objects\n",
    "start_date_datetime = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "end_date_datetime = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "\n",
    "# Set log file \n",
    "SaveLogsTo = os.path.join(download_directory, \"Download_Log\")\n",
    "\n",
    "#Setup logging - levels are DEBUG,INFO,WARNING,ERROR,CRITICAL\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "''' ********************** SCRIPT CONFIGURATION END ********************** '''\n",
    "\n",
    "#https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist\n",
    "def createFolder(folderPath):\n",
    "    if not os.path.exists(folderPath):\n",
    "        try:\n",
    "            os.makedirs(folderPath)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "#Create specified folder if it does not exist already\n",
    "createFolder(SaveLogsTo)\n",
    "\n",
    "#Logging level specified in script configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logFileName = datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "fileHandler = logging.handlers.RotatingFileHandler('{}/{}.log'.format(SaveLogsTo, logFileName), maxBytes=100000, backupCount=5)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(relativeCreated)d \\n%(filename)s %(module)s %(funcName)s %(lineno)d \\n%(message)s\\n')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "offset = 0 # offset used to run with response query index offset - usually when download request times out or disconnects\n",
    "items_to_download =[]\n",
    "item_responses = 0\n",
    "\n",
    "# Get the spatial extent of the feature class\n",
    "desc = arcpy.Describe(feature_class)\n",
    "spatial_extent = desc.extent\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin = spatial_extent.XMin\n",
    "ymin = spatial_extent.YMin\n",
    "xmax = spatial_extent.XMax\n",
    "ymax = spatial_extent.YMax\n",
    "\n",
    "print(xmin, ymin, xmax, ymax)\n",
    "\n",
    "extent_dd = spatial_extent.projectAs(arcpy.SpatialReference(4326))\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin1 = extent_dd.XMin\n",
    "ymin1 = extent_dd.YMin\n",
    "xmax1 = extent_dd.XMax\n",
    "ymax1 = extent_dd.YMax\n",
    "\n",
    "print(xmin1, ymin1, xmax1, ymax1)\n",
    "\n",
    "params = {\n",
    "    \"bbox\": f\"{xmin1},{ymin1},{xmax1},{ymax1}\"\n",
    "}\n",
    "\n",
    "logger.info(\"Initiating API Query:: {0}\".format(str(datetime.now())))\n",
    "while True:\n",
    "    # Create the API URL with the current offset\n",
    "    url = f\"{url}&offset={offset}\"\n",
    "\n",
    "    # Send the API request and retrieve the response\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()[\"items\"]\n",
    "\n",
    "        # If there are no items in the response, we have retrieved all items\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "\n",
    "        for index, item in enumerate(data):\n",
    "            item_responses += 1\n",
    "            pub_date = datetime.strptime(item[\"publicationDate\"], \"%Y-%m-%d\")\n",
    "            download_url = item[\"downloadURL\"]\n",
    "            if start_date_datetime <= pub_date <= end_date_datetime:\n",
    "                if download_url not in [item[\"download_url\"] for item in items_to_download]: # only unique instances of the download url will be added to the dictionary from the query results\n",
    "                    items_to_download.append({\n",
    "                        \"publication_date\": item[\"publicationDate\"],\n",
    "                        \"download_url\": item[\"downloadURL\"],\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"response_index\": index + 1})\n",
    "            \n",
    "        # Increment the offset for the next page of items\n",
    "        offset += 999\n",
    "\n",
    "logger.info(f\"Total items returned from API Query: {item_responses}\")\n",
    "logger.info(f\"Total items added to download list: {len(items_to_download)}\")\n",
    "\n",
    "# Iterate over each item in the items to download list and download them\n",
    "logger.info(\"Initiating TIFF file download from USGS:: {0}\".format(datetime.now()))\n",
    "for index, item in enumerate(items_to_download):\n",
    "    publication_date = item[\"publication_date\"]\n",
    "    download_url = item[\"download_url\"]\n",
    "    title = item[\"title\"]\n",
    "    response = item[\"response_index\"]\n",
    "\n",
    "    # Specify the local file path to save the downloaded file\n",
    "    rename = title.replace(\" \",\"_\") # replace spaces in name with underscores\n",
    "    renamex = rename.replace(\"/\",\"_\")\n",
    "    save_path = os.path.join(download_directory, renamex + \".tif\")\n",
    "\n",
    "        # Download the file and save it locally\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(requests.get(download_url).content)\n",
    "        logger.info(f\"saved item : : \\nTitle: {title}\\n{index + 1} of {len(items_to_download)} items to download\\nRespone Query Index : {response}\")\n",
    "        arcpy.SetProgressorPosition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USGS API Query for 1m DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initiating API Query:: 2023-12-12 13:16:31.661130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2676837.2796126306 825761.5556894839 2728988.418748464 871059.5494109006\n",
      "-76.7117083647688 35.993993796206844 -76.53165547368441 36.12182421674153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total items returned from API Query: 0\n",
      "INFO:__main__:Total items added to download list: 0\n",
      "INFO:__main__:Initiating TIFF file download from USGS:: 2023-12-12 13:16:33.373237\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import requests\n",
    "import logging\n",
    "import errno\n",
    "import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from datetime import datetime\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://tnmaccess.nationalmap.gov/api/v1/products?datasets=Digital%20Elevation%20Model%20(DEM)%201%20meter&max=999000\"\n",
    "\n",
    "# Define the query parameters\n",
    "\n",
    "# Set the feature class or shapefile path\n",
    "\n",
    "feature_class = r\"C:\\Project_Files\\GIS\\HMP+\\Edenton\\4.0 Reference Data\\USGS\\LiDAR_Processing\\Working_Data\\Other_Data\\edenton_bb.shp\"\n",
    "\n",
    "# Set the file download Directory\n",
    "download_directory = r\"C:\\Project_Files\\GIS\\HMP+\\Edenton\\4.0 Reference Data\\USGS\\LiDAR_Processing\\Working_Data\\Working_DEM\\01m\"\n",
    "\n",
    "# Define the date range (start date and end date)\n",
    "start_date_str = \"2010-01-01\"\n",
    "end_date_str = \"2023-05-01\"\n",
    "\n",
    "# Convert start_date_str and end_date_str to datetime objects\n",
    "start_date_datetime = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "end_date_datetime = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
    "\n",
    "# Set log file \n",
    "SaveLogsTo = os.path.join(download_directory, \"Download_Log\")\n",
    "\n",
    "#Setup logging - levels are DEBUG,INFO,WARNING,ERROR,CRITICAL\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "''' ********************** SCRIPT CONFIGURATION END ********************** '''\n",
    "\n",
    "#https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist\n",
    "def createFolder(folderPath):\n",
    "    if not os.path.exists(folderPath):\n",
    "        try:\n",
    "            os.makedirs(folderPath)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "#Create specified folder if it does not exist already\n",
    "createFolder(SaveLogsTo)\n",
    "\n",
    "#Logging level specified in script configuration\n",
    "logger = logging.getLogger(__name__)\n",
    "logFileName = datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "fileHandler = logging.handlers.RotatingFileHandler('{}/{}.log'.format(SaveLogsTo, logFileName), maxBytes=100000, backupCount=5)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(relativeCreated)d \\n%(filename)s %(module)s %(funcName)s %(lineno)d \\n%(message)s\\n')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "offset = 0 # offset used to run with response query index offset - usually when download request times out or disconnects\n",
    "items_to_download =[]\n",
    "item_responses = 0\n",
    "\n",
    "# Get the spatial extent of the feature class\n",
    "desc = arcpy.Describe(feature_class)\n",
    "spatial_extent = desc.extent\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin = spatial_extent.XMin\n",
    "ymin = spatial_extent.YMin\n",
    "xmax = spatial_extent.XMax\n",
    "ymax = spatial_extent.YMax\n",
    "\n",
    "print(xmin, ymin, xmax, ymax)\n",
    "\n",
    "extent_dd = spatial_extent.projectAs(arcpy.SpatialReference(4326))\n",
    "\n",
    "# Extract the extent properties\n",
    "xmin1 = extent_dd.XMin\n",
    "ymin1 = extent_dd.YMin\n",
    "xmax1 = extent_dd.XMax\n",
    "ymax1 = extent_dd.YMax\n",
    "\n",
    "print(xmin1, ymin1, xmax1, ymax1)\n",
    "\n",
    "params = {\n",
    "    \"bbox\": f\"{xmin1},{ymin1},{xmax1},{ymax1}\"\n",
    "}\n",
    "\n",
    "logger.info(\"Initiating API Query:: {0}\".format(str(datetime.now())))\n",
    "while True:\n",
    "    # Create the API URL with the current offset\n",
    "    url = f\"{url}&offset={offset}\"\n",
    "\n",
    "    # Send the API request and retrieve the response\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()[\"items\"]\n",
    "\n",
    "        # If there are no items in the response, we have retrieved all items\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "\n",
    "        for index, item in enumerate(data):\n",
    "            item_responses += 1\n",
    "            pub_date = datetime.strptime(item[\"publicationDate\"], \"%Y-%m-%d\")\n",
    "            download_url = item[\"downloadURL\"]\n",
    "            if start_date_datetime <= pub_date <= end_date_datetime:\n",
    "                if download_url not in [item[\"download_url\"] for item in items_to_download]: # only unique instances of the download url will be added to the dictionary from the query results\n",
    "                    items_to_download.append({\n",
    "                        \"publication_date\": item[\"publicationDate\"],\n",
    "                        \"download_url\": item[\"downloadURL\"],\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"response_index\": index + 1})\n",
    "            \n",
    "        # Increment the offset for the next page of items\n",
    "        offset += 999\n",
    "\n",
    "logger.info(f\"Total items returned from API Query: {item_responses}\")\n",
    "logger.info(f\"Total items added to download list: {len(items_to_download)}\")\n",
    "\n",
    "# Iterate over each item in the items to download list and download them\n",
    "logger.info(\"Initiating TIFF file download from USGS:: {0}\".format(datetime.now()))\n",
    "for index, item in enumerate(items_to_download):\n",
    "    publication_date = item[\"publication_date\"]\n",
    "    download_url = item[\"download_url\"]\n",
    "    title = item[\"title\"]\n",
    "    response = item[\"response_index\"]\n",
    "\n",
    "    # Specify the local file path to save the downloaded file\n",
    "    rename = title.replace(\" \",\"_\") # replace spaces in name with underscores\n",
    "    renamex = rename.replace(\"/\",\"_\")\n",
    "    save_path = os.path.join(download_directory, renamex + \".tif\")\n",
    "\n",
    "        # Download the file and save it locally\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        file.write(requests.get(download_url).content)\n",
    "        logger.info(f\"saved item : : \\nTitle: {title}\\n{index + 1} of {len(items_to_download)} items to download\\nRespone Query Index : {response}\")\n",
    "        arcpy.SetProgressorPosition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
